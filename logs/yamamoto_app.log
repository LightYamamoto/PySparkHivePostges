2023-02-15 19:09:03,914 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:54 - Message:Running Pipeline 1
2023-02-15 19:09:09,366 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:31 - Message:Running Pipeline
2023-02-15 19:09:24,997 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:48 - Message:An error ocured while running the pipeline >HDFS Directory already exists
2023-02-15 19:28:15,493 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:54 - Message:Running Pipeline 1
2023-02-15 19:28:21,535 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:31 - Message:Running Pipeline
2023-02-15 19:28:37,394 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:48 - Message:An error ocured while running the pipeline >HDFS Directory already exists
2023-02-16 16:12:20,281 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:54 - Message:Running Pipeline 1
2023-02-16 16:12:26,284 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:31 - Message:Running Pipeline
2023-02-16 16:12:42,029 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:48 - Message:An error ocured while running the pipeline >HDFS Directory already exists
2023-02-16 19:53:50,853 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:58 - Message:Running Pipeline 1
2023-02-16 19:54:32,694 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:58 - Message:Running Pipeline 1
2023-02-16 20:04:05,588 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message:Running Pipeline 1
2023-02-16 20:04:54,542 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message:Running Pipeline 1
2023-02-16 20:06:10,711 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message:Running Pipeline 1
2023-02-16 20:08:24,624 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:63 - Message:Running Pipeline 1
2023-02-16 20:08:30,202 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:08:45,572 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:10:08,128 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:66 - Message:Running Pipeline 1
2023-02-16 20:10:13,688 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:10:13,688 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:10:13,688 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:13:36,082 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:66 - Message:Running Pipeline 1
2023-02-16 20:13:41,661 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:13:56,882 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:13:56,884 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:14:43,357 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:66 - Message:Running Pipeline 1
2023-02-16 20:14:48,889 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:15:04,081 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:15:04,081 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:16:12,688 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:66 - Message:Running Pipeline 1
2023-02-16 20:16:18,513 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:16:34,182 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:16:34,182 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:17:03,517 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message:Running Pipeline 1
2023-02-16 20:17:09,100 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:17:24,270 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:17:24,270 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:19:04,817 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message:Running Pipeline 1
2023-02-16 20:19:10,363 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:19:25,652 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:19:25,652 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:20:31,747 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message:Running Pipeline 1
2023-02-16 20:20:37,331 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:22:02,671 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message:Running Pipeline 1
2023-02-16 20:22:08,258 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:22:26,145 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:59 - Message: Hive table had been created
2023-02-16 20:22:26,146 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:61 - Message: Querying from Hive Database
2023-02-16 20:25:15,788 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-16 20:25:21,327 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:55 - Message: Create_hive_table methob started
2023-02-16 20:25:48,840 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:71 - Message: Hive table had been created
2023-02-16 20:25:48,840 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:73 - Message: Querying from Hive Database
2023-02-16 20:28:09,900 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:82 - Message:Running Pipeline 1
2023-02-16 20:32:16,307 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:82 - Message:Running Pipeline 1
2023-02-17 09:24:39,193 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-17 09:24:39,193 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:23 - Message:Running Pipeline
2023-02-17 09:24:57,445 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:39 - Message:An error throw while running the pipeline >An error occurred while calling o28.showString.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/Yamamoto/PycharmProjects/SparkAppDemo2/spark-warehouse/yamamoto_db.db/yamamoto_table
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Input path does not exist: file:/Users/Yamamoto/PycharmProjects/SparkAppDemo2/spark-warehouse/yamamoto_db.db/yamamoto_table
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)
	... 52 more

2023-02-17 09:51:24,486 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-17 09:51:24,486 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:23 - Message:Running Pipeline
2023-02-17 09:51:42,152 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:39 - Message:An error throw while running the pipeline >An error occurred while calling o28.showString.
: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/Yamamoto/PycharmProjects/SparkAppDemo2/spark-warehouse/yamamoto_db.db/yamamoto_table
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Input path does not exist: file:/Users/Yamamoto/PycharmProjects/SparkAppDemo2/spark-warehouse/yamamoto_db.db/yamamoto_table
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)
	... 52 more

2023-02-17 09:59:02,424 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-17 09:59:02,424 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:23 - Message:Running Pipeline
2023-02-17 09:59:17,946 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:39 - Message:An error throw while running the pipeline >Table or view not found: yamamoto_db.yamamoto_table; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [yamamoto_db, yamamoto_table], [], false

2023-02-17 10:03:03,835 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-17 10:03:03,835 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:52 - Message: Create_hive_table methob started
2023-02-17 10:03:37,997 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-17 10:03:43,502 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:52 - Message: Create_hive_table methob started
2023-02-17 10:04:09,905 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:72 - Message: Hive table had been created
2023-02-17 10:04:09,905 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:23 - Message:Running Pipeline
2023-02-17 16:50:50,423 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:74 - Message:Running Pipeline 1
2023-02-17 16:50:56,363 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:47 - Message: Create_hive_table methob started
2023-02-17 16:51:22,604 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message: Hive table had been created
2023-02-17 16:51:22,605 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-17 16:53:53,595 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:74 - Message:Running Pipeline 1
2023-02-17 16:53:59,125 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:47 - Message: Create_hive_table methob started
2023-02-17 16:54:24,094 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message: Hive table had been created
2023-02-17 16:54:24,094 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-17 16:54:24,094 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:34 - Message:An error throw while running the pipeline >name 'df' is not defined
2023-02-17 16:56:30,036 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:74 - Message:Running Pipeline 1
2023-02-17 16:56:35,574 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:47 - Message: Create_hive_table methob started
2023-02-17 16:56:59,823 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message: Hive table had been created
2023-02-17 16:56:59,823 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 00:03:56,147 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:74 - Message:Running Pipeline 1
2023-02-18 00:04:02,263 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:47 - Message: Create_hive_table methob started
2023-02-18 00:04:28,169 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:67 - Message: Hive table had been created
2023-02-18 00:04:28,170 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 10:45:23,936 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:77 - Message:Running Pipeline 1
2023-02-18 10:45:29,925 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:50 - Message: Create_hive_table methob started
2023-02-18 10:45:56,074 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:70 - Message: Hive table had been created
2023-02-18 10:45:56,075 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 11:03:07,209 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-18 11:03:12,802 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:52 - Message: Create_hive_table methob started
2023-02-18 11:03:37,927 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:72 - Message: Hive table had been created
2023-02-18 11:03:37,928 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 11:03:37,974 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:39 - Message:An error throw while running the pipeline >duplicate key value violates unique constraint "yamamoto_table_pkey"
DETAIL:  Key (course_id)=(3) already exists.

2023-02-18 11:04:47,472 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-18 11:04:53,101 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:52 - Message: Create_hive_table methob started
2023-02-18 11:05:18,525 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:72 - Message: Hive table had been created
2023-02-18 11:05:18,525 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 11:06:17,676 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:79 - Message:Running Pipeline 1
2023-02-18 11:06:23,245 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 14:16:34,295 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:88 - Message:Running Pipeline 1
2023-02-18 15:07:42,009 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:88 - Message:Running Pipeline 1
2023-02-18 18:23:46,888 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:90 - Message:Running Pipeline 1
2023-02-18 18:23:53,262 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 18:23:54,657 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:41 - Message:An error throw while running the pipeline >Option 'dbtable' or 'query' is required.
2023-02-18 18:26:38,849 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:90 - Message:Running Pipeline 1
2023-02-18 18:26:44,840 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 18:26:46,057 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:41 - Message:An error throw while running the pipeline >An error occurred while calling o36.load.
: java.sql.SQLException: No suitable driver
	at java.sql.DriverManager.getDriver(DriverManager.java:315)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)

2023-02-18 18:27:28,389 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:90 - Message:Running Pipeline 1
2023-02-18 18:27:34,317 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 18:27:35,478 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:41 - Message:An error throw while running the pipeline >An error occurred while calling o36.load.
: java.sql.SQLException: No suitable driver
	at java.sql.DriverManager.getDriver(DriverManager.java:315)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)

2023-02-18 18:28:32,687 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:90 - Message:Running Pipeline 1
2023-02-18 18:28:38,323 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 18:28:39,798 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:41 - Message:An error throw while running the pipeline >An error occurred while calling o35.load.
: org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:253)
	at org.postgresql.Driver.makeConnection(Driver.java:434)
	at org.postgresql.Driver.connect(Driver.java:291)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:242)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: username:password@localhost
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
	at org.postgresql.core.PGStream.<init>(PGStream.java:98)
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)
	... 30 more

2023-02-18 18:31:30,849 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:91 - Message:Running Pipeline 1
2023-02-18 18:31:36,435 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 18:31:37,867 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:41 - Message:An error throw while running the pipeline >An error occurred while calling o33.load.
: org.postgresql.util.PSQLException: The connection attempt failed.
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:354)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:253)
	at org.postgresql.Driver.makeConnection(Driver.java:434)
	at org.postgresql.Driver.connect(Driver.java:291)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
	at org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:122)
	at org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:118)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:63)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:242)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.UnknownHostException: postgres:1011997@localhost
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:607)
	at org.postgresql.core.PGStream.createSocket(PGStream.java:243)
	at org.postgresql.core.PGStream.<init>(PGStream.java:98)
	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:132)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:258)
	... 30 more

2023-02-18 18:32:16,222 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:91 - Message:Running Pipeline 1
2023-02-18 18:32:21,973 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:10:30,160 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:97 - Message:Running Pipeline 1
2023-02-18 22:10:34,975 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:10:41,994 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:47 - Message:An error throw while running the pipeline >name 'transform_df' is not defined
2023-02-18 22:11:20,074 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:97 - Message:Running Pipeline 1
2023-02-18 22:11:24,430 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:14:23,119 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:14:27,238 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:14:47,844 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:49 - Message:An error throw while running the pipeline >Table or view 'yamamoto_schema.course_table' already exists. SaveMode: ErrorIfExists.
2023-02-18 22:15:13,605 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:15:18,053 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:15:20,789 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:49 - Message:An error throw while running the pipeline >An error occurred while calling o29.sql
2023-02-18 22:15:47,240 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:15:51,579 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:17:06,977 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:17:11,357 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:17:12,618 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:49 - Message:An error throw while running the pipeline >An error occurred while calling o35.load.
: org.postgresql.util.PSQLException: ERROR: relation "yamamoto_schema.yamamoto_table1" does not exist
  Position: 15
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:356)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:496)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:413)
	at org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:190)
	at org.postgresql.jdbc.PgPreparedStatement.executeQuery(PgPreparedStatement.java:134)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:68)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:242)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:37)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)

2023-02-18 22:17:35,950 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:17:40,227 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:17:47,269 - Logger:sLogger - Level:ERROR - Module:data_pipeline - Line:49 - Message:An error throw while running the pipeline >'PipeLine' object has no attribute 'persist'
2023-02-18 22:18:25,919 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:18:30,216 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
2023-02-18 22:19:51,201 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:99 - Message:Running Pipeline 1
2023-02-18 22:19:55,560 - Logger:sLogger - Level:INFO - Module:data_pipeline - Line:17 - Message:Running Pipeline
